{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46eaf841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, Dense, LSTM, Dropout, Embedding, Activation\n",
    "from keras.layers import concatenate, BatchNormalization, Input\n",
    "#from keras.layers.merge import add\n",
    "from keras.layers import concatenate\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "import matplotlib.pyplot as plt  # for plotting data\n",
    "import cv2\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63bbe71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A child in a pink dress is climbing up a set of stairs in an entry way .', 'A girl going into a wooden building .', 'A little girl climbing into a wooden playhouse .', 'A little girl climbing the stairs to her playhouse .', 'A little girl in a pink dress going into a wooden cabin .']\n"
     ]
    }
   ],
   "source": [
    "def load_description(text):\n",
    "    mapping = dict()\n",
    "    for line in text.split(\"\\n\"):\n",
    "        token = line.split(\"\\t\")\n",
    "        if len(line) < 2:   # remove short descriptions\n",
    "            continue\n",
    "        img_id = token[0].split('.')[0] # name of the image\n",
    "        img_des = token[1]              # description of the image\n",
    "        if img_id not in mapping:\n",
    "            mapping[img_id] = list()\n",
    "        mapping[img_id].append(img_des)\n",
    "    return mapping\n",
    "  \n",
    "token_path = 'C:/Users/ADMIN/Downloads/archive (2)/Flickr_Data/Flickr_Data/Flickr_TextData/Flickr8k.token.txt'\n",
    "text = open(token_path, 'r', encoding = 'utf-8').read()\n",
    "descriptions = load_description(text)\n",
    "print(descriptions['1000268201_693b08cb0e'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b04f31d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['child in pink dress is climbing up set of stairs in an entry way',\n",
       " 'girl going into wooden building',\n",
       " 'little girl climbing into wooden playhouse',\n",
       " 'little girl climbing the stairs to her playhouse',\n",
       " 'little girl in pink dress going into wooden cabin']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_description(desc):\n",
    "    for key, des_list in desc.items():\n",
    "        for i in range(len(des_list)):\n",
    "            caption = des_list[i]\n",
    "            caption = [ch for ch in caption if ch not in string.punctuation]\n",
    "            caption = ''.join(caption)\n",
    "            caption = caption.split(' ')\n",
    "            caption = [word.lower() for word in caption if len(word)>1 and word.isalpha()]\n",
    "            caption = ' '.join(caption)\n",
    "            des_list[i] = caption\n",
    "  \n",
    "clean_description(descriptions)\n",
    "descriptions['1000268201_693b08cb0e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1581b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_vocab(desc):\n",
    "    words = set()\n",
    "    for key in desc.keys():\n",
    "        for line in desc[key]:\n",
    "            words.update(line.split())\n",
    "    return words\n",
    "vocab = to_vocab(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68561caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['startseq child in pink dress is climbing up set of stairs in an entry way endseq', 'startseq girl going into wooden building endseq', 'startseq little girl climbing into wooden playhouse endseq', 'startseq little girl climbing the stairs to her playhouse endseq', 'startseq little girl in pink dress going into wooden cabin endseq']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "images = 'C:/Users/ADMIN/Downloads/archive (2)/Flickr_Data/Flickr_Data/Images'\n",
    "# Create a list of all image names in the directory\n",
    "img = glob.glob(images + '*.jpg')\n",
    "  \n",
    "train_path = 'C:/Users/ADMIN/Downloads/archive (2)/Flickr_Data/Flickr_Data/Flickr_TextData/Flickr_8k.trainImages.txt'\n",
    "train_images = open(train_path, 'r', encoding = 'utf-8').read().split(\"\\n\")\n",
    "train_img = []  # list of all images in training set\n",
    "for im in img:\n",
    "    if(im[len(images):] in train_images):\n",
    "        train_img.append(im)\n",
    "          \n",
    "# load descriptions of training set in a dictionary. Name of the image will act as key\n",
    "def load_clean_descriptions(des, dataset):\n",
    "    dataset_des = dict()\n",
    "    for key, des_list in des.items():\n",
    "        if key+'.jpg' in dataset:\n",
    "            if key not in dataset_des:\n",
    "                dataset_des[key] = list()\n",
    "            for line in des_list:\n",
    "                desc = 'startseq ' + line + ' endseq'\n",
    "                dataset_des[key].append(desc)\n",
    "    return dataset_des\n",
    "\n",
    "train_descriptions = load_clean_descriptions(descriptions, train_images)\n",
    "print(train_descriptions['1000268201_693b08cb0e'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f93dd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n",
      "96112376/96112376 [==============================] - 20s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "def preprocess_img(img_path):\n",
    "    # inception v3 excepts img in 299 * 299 * 3\n",
    "    img = load_img(img_path, target_size = (299, 299))\n",
    "    x = img_to_array(img)\n",
    "    # Add one more dimension\n",
    "    x = np.expand_dims(x, axis = 0)\n",
    "    x = preprocess_input(x)\n",
    "    return x\n",
    "  \n",
    "def encode(image):\n",
    "    image = preprocess_img(image)\n",
    "    vec = model.predict(image)\n",
    "    vec = np.reshape(vec, (vec.shape[1]))\n",
    "    return vec\n",
    "  \n",
    "base_model = InceptionV3(weights = 'imagenet')\n",
    "model = Model(base_model.input, base_model.layers[-2].output)\n",
    "# run the encode function on all train images and store the feature vectors in a list\n",
    "encoding_train = {}\n",
    "for img in train_img:\n",
    "    encoding_train[img[len(images):]] = encode(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1d15e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of all training captions\n",
    "all_train_captions = []\n",
    "for key, val in train_descriptions.items():\n",
    "    for caption in val:\n",
    "        all_train_captions.append(caption)\n",
    "  \n",
    "# consider only words which occur atleast 10 times\n",
    "vocabulary = vocab\n",
    "threshold = 10 # you can change this value according to your need\n",
    "word_counts = {}\n",
    "for cap in all_train_captions:\n",
    "    for word in cap.split(' '):\n",
    "        word_counts[word] = word_counts.get(word, 0) + 1\n",
    "  \n",
    "vocab = [word for word in word_counts if word_counts[word] >= threshold]\n",
    "  \n",
    "# word mapping to integers\n",
    "ixtoword = {}\n",
    "wordtoix = {}\n",
    "  \n",
    "ix = 1\n",
    "for word in vocab:\n",
    "    wordtoix[word] = ix\n",
    "    ixtoword[ix] = word\n",
    "    ix += 1\n",
    "      \n",
    "# find the maximum length of a description in a dataset\n",
    "max_length = max(len(des.split()) for des in all_train_captions)\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d985d3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m X1, X2, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(), \u001b[38;5;28mlist\u001b[39m(), \u001b[38;5;28mlist\u001b[39m()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, des_list \u001b[38;5;129;01min\u001b[39;00m train_descriptions\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m----> 3\u001b[0m     pic \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_features\u001b[49m[key \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cap \u001b[38;5;129;01min\u001b[39;00m des_list:\n\u001b[0;32m      5\u001b[0m         seq \u001b[38;5;241m=\u001b[39m [wordtoix[word] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m cap\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m wordtoix]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_features' is not defined"
     ]
    }
   ],
   "source": [
    "X1, X2, y = list(), list(), list()\n",
    "for key, des_list in train_descriptions.items():\n",
    "    pic = train_features[key + '.jpg']\n",
    "    for cap in des_list:\n",
    "        seq = [wordtoix[word] for word in cap.split(' ') if word in wordtoix]\n",
    "        for i in range(1, len(seq)):\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            in_seq = pad_sequences([in_seq], maxlen = max_length)[0]\n",
    "            out_seq = to_categorical([out_seq], num_classes = vocab_size)[0]\n",
    "            # store\n",
    "            X1.append(pic)\n",
    "            X2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "  \n",
    "X2 = np.array(X2)\n",
    "X1 = np.array(X1)\n",
    "y = np.array(y)\n",
    "  \n",
    "# load glove vectors for embedding layer\n",
    "embeddings_index = {}\n",
    "golve_path ='/kaggle / input / glove-global-vectors-for-word-representation / glove.6B.200d.txt'\n",
    "glove = open(golve_path, 'r', encoding = 'utf-8').read()\n",
    "for line in glove.split(\"\\n\"):\n",
    "    values = line.split(\" \")\n",
    "    word = values[0]\n",
    "    indices = np.asarray(values[1: ], dtype = 'float32')\n",
    "    embeddings_index[word] = indices\n",
    "    \n",
    "emb_dim = 200\n",
    "emb_matrix = np.zeros((vocab_size, emb_dim))\n",
    "for word, i in wordtoix.items():\n",
    "    emb_vec = embeddings_index.get(word)\n",
    "    if emb_vec is not None:\n",
    "        emb_matrix[i] = emb_vec\n",
    "emb_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46b2570",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
